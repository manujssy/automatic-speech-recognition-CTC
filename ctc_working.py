# -*- coding: utf-8 -*-
"""CTC working.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gRfxIMQJbdIrkUOGzMCnWunmieotzQ_
"""

!pip install jiwer

import torch
import torchaudio
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import os
from jiwer import wer  # For Word Error Rate calculation

# 1. Model and Processor
model_name = "facebook/wav2vec2-base-960h"  # Or a larger variant
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForCTC.from_pretrained(model_name)

# 2. Audio Transcription (with Chunking for long audio)
def transcribe_audio(audio_path, chunk_length_seconds=10):  # Chunking added
    try:
        waveform, sample_rate = torchaudio.load(audio_path)
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)

        chunk_size = int(chunk_length_seconds * 16000)
        num_chunks = (len(waveform[0]) + chunk_size - 1) // chunk_size
        all_transcriptions = []

        for i in range(num_chunks):
            start = i * chunk_size
            end = min((i + 1) * chunk_size, len(waveform[0]))
            chunk = waveform[:, start:end]

            input_values = processor(chunk.squeeze(), return_tensors="pt").input_values
            with torch.no_grad():
                logits = model(input_values).logits

            predicted_ids = torch.argmax(logits, dim=-1)
            transcription = processor.batch_decode(predicted_ids)[0]
            all_transcriptions.append(transcription)

        full_transcription = " ".join(all_transcriptions)  # Basic combining - improve if needed
        return full_transcription

    except Exception as e:
        print(f"Error transcribing {audio_path}: {e}")
        return None



# 3. Evaluation (Word Error Rate)
def calculate_wer(ground_truth, predicted):
    if ground_truth is None or predicted is None:
        return 1.0  # Or another appropriate value for error

    return wer(ground_truth, predicted)  # jiwer handles lowercasing and splitting


def calculate_metrics(ground_truth, predicted):
    if ground_truth is None or predicted is None:
        return {'precision': 0, 'recall': 0, 'f1_score': 0, 'accuracy': 0}

    ground_truth_words = ground_truth.lower().split()
    predicted_words = predicted.lower().split()

    common_words = set(ground_truth_words) & set(predicted_words)
    precision = len(common_words) / len(predicted_words) if predicted_words else 0
    recall = len(common_words) / len(ground_truth_words) if ground_truth_words else 0
    f1 = 2 * precision * recall / (precision + recall) if precision + recall else 0
    accuracy = len(common_words) / len(set(ground_truth_words) | set(predicted_words)) if (set(ground_truth_words) | set(predicted_words)) else 0
    return {'precision': precision, 'recall': recall, 'f1_score': f1, 'accuracy': accuracy}



# 4. Dataset and Evaluation Loop (with weights and metrics)
audio_files_and_labels = [
    ("sounds/Life is a beautiful (alfred-british).wav", "Life is a beautiful journey", 1),
    ("sounds/Hello Myself Devajit.wav", "hello myself devajit", 1),
    ("sounds/Hello My self Ryan(canadian -ryan).wav", "hello myself ryan", 1),
    ("sounds/Hello My self Rishi(indian-rishi).wav", "hello myself rishi", 1),
    ("sounds/Hello Myself Devajit (irish-cillian).wav", "hello myself Devajit", 1),
    ("sounds/Life is a beautiful (shirley-scottish).wav", "Life is a beautiful journey", 1),
]

total_wer = 0
total_precision = 0
total_recall = 0
total_f1 = 0
total_accuracy = 0
total_weight = 0
num_transcriptions = 0

for audio_path, ground_truth, weight in audio_files_and_labels:
    if os.path.exists(audio_path):
        transcription = transcribe_audio(audio_path)
        if transcription:
            # ... (print audio path, ground truth, and transcription)

            wer_value = calculate_wer(ground_truth, transcription)
            # ... (print WER)

            metrics = calculate_metrics(ground_truth, transcription)  # Calculate metrics
            total_precision += metrics['precision'] * weight
            total_recall += metrics['recall'] * weight
            total_f1 += metrics['f1_score'] * weight
            total_accuracy += metrics['accuracy'] * weight
            total_wer += wer_value * weight
            total_weight += weight
            num_transcriptions += 1
        else:
            print(f"Transcription failed for: {audio_path}")
    else:
        print(f"File not found: {audio_path}")

if num_transcriptions > 0:
    weighted_average_wer = total_wer / total_weight if total_weight > 0 else 0
    weighted_average_precision = total_precision / total_weight if total_weight > 0 else 0
    weighted_average_recall = total_recall / total_weight if total_weight > 0 else 0
    weighted_average_f1 = total_f1 / total_weight if total_weight > 0 else 0
    weighted_average_accuracy = total_accuracy / total_weight if total_weight > 0 else 0

    print(f"\nWeighted Averages:")
    print(f"Precision: {weighted_average_precision:.4f}")
    print(f"Recall: {weighted_average_recall:.4f}")
    print(f"F1-score: {weighted_average_f1:.4f}")
    print(f"Accuracy: {weighted_average_accuracy:.4f}")


else:
    print("No successful transcriptions to calculate weighted averages.")